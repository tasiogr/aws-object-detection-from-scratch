{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Object Detection\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Shuffle and Split Manifest](#Shuffle-and-Split-Manifest)\n",
    "  2. [Create Channels](#Create-Channels)\n",
    "4. [Training](#Initial-Training)\n",
    "6. [Hosting](#Hosting)\n",
    "7. [Inference](#Inference)\n",
    "8. [Preparation for AWS DeepLens](#Preparation-for-AWS-DeepLens)\n",
    "  1. [Deploy](#Deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, I will show you how to train an object detector using an augmented manifest generated by a GroundTruth Labeling Job.\n",
    "\n",
    "Then, I will deploy and test the model with a SageMaker Endpoint, and prepare the model for deploying it to DeepLens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the Object Detection algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3 will automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that you want to use for training and to store the trained model artifacts. In this notebook, we require a custom bucket that exists so as to keep the naming clean. You can end up using a default bucket that SageMaker comes with as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your-labeled-data-bucket-name>'\n",
    "\n",
    "manifest_key = '<your-ground-truth-output-manifest-key>' #e.g.: 'labeled/nfingers/manifests/output/output.manifest'\n",
    "\n",
    "s3_artifact_location = 's3://{}/output'.format(bucket)\n",
    "\n",
    "training_manifest_key = 'labeled/nfingers/manifests/output/training_manifest.json'\n",
    "validation_manifest_key = 'labeled/nfingers/manifests/output/validation_manifest.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the Amazon SageMaker Object Detection docker image, which is static and need not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The current manifest file includes all the data that was labeled by the Labeling Job in GroundTruth. In order to pass it through the algorithm, we need to shuffle it and split it into training and validation data sets. These data sets are then passed to the SageMaker Estimator as channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "manifest = s3.Object(bucket, manifest_key)\n",
    "items = manifest.get()['Body'].read().decode('utf-8').split('\\n')\n",
    "\n",
    "training_filename = 'training.manifest'\n",
    "validation_filename = 'validation.manifest'\n",
    "\n",
    "data = np.array(items)\n",
    "# Before splitting, it's good to shuffle the entire dataset.\n",
    "\n",
    "np.random.shuffle(data)\n",
    "training_set, validation_set = train_test_split(data, test_size=0.2)\n",
    "\n",
    "with open(training_filename, mode='w') as manifest:\n",
    "    for item in training_set:\n",
    "        if item:\n",
    "            manifest.write(item + os.linesep)\n",
    "    \n",
    "with open(validation_filename, mode='w') as manifest:\n",
    "    for item in validation_set:\n",
    "        if item:\n",
    "            manifest.write(item + os.linesep)\n",
    "        \n",
    "training_manifest = s3.Object(bucket, training_manifest_key)\n",
    "training_manifest.upload_file(training_filename)\n",
    "\n",
    "validation_manifest = s3.Object(bucket, validation_manifest_key)\n",
    "validation_manifest.upload_file(validation_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create channels\n",
    "Let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_uri(obj):\n",
    "    return 's3://{}/{}'.format(obj.bucket_name, obj.key)\n",
    "\n",
    "# Create a train and validation data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "training_data = sagemaker.session.s3_input(\n",
    "                                        get_object_uri(training_manifest),\n",
    "                                        distribution='FullyReplicated',\n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'nfingers'],\n",
    "                                        record_wrapping='RecordIO') \n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "                                        get_object_uri(validation_manifest),\n",
    "                                        distribution='FullyReplicated',\n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'nfingers'],\n",
    "                                        record_wrapping='RecordIO')\n",
    "\n",
    "data_channels = {'train': training_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/model/output'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a `sagemaker.estimator.Estimator` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'Pipe',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detection algorithm at its core is the [Single-Shot Multi-Box detection algorithm (SSD)](https://arxiv.org/abs/1512.02325). This algorithm uses a `base_network`, which is typically a [VGG](https://arxiv.org/abs/1409.1556) or a [ResNet](https://arxiv.org/abs/1512.03385). The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50 now. It also has a lot of options for hyperparameters that help configure the training job. The next step in our training, is to setup these hyperparameters and data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Object Detection [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=3,\n",
    "                             mini_batch_size=4,\n",
    "                             epochs=50,\n",
    "                             learning_rate=0.001,\n",
    "                             lr_scheduler_step='3,6',\n",
    "                             lr_scheduler_factor=0.1,\n",
    "                             optimizer='rmsprop',\n",
    "                             momentum=0.9,\n",
    "                             weight_decay=0.0005,\n",
    "                             overlap_threshold=0.5,\n",
    "                             nms_threshold=0.45,\n",
    "                             image_shape=300,\n",
    "                             label_width=100,\n",
    "                             num_training_samples=len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyperparameters for this object and we have our data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the `Estimator` classes are provisioned and are setup with the appropriate libraries. Then, thanks to using `Pipe` mode, the data from our channels is streamed into the instance and the training begins immediately after the training image is downloaded to the instance and we'll start getting logs. The data logs will also print out Mean Average Precision (mAP) on the validation data, among other losses, for every run of the dataset once or one epoch. This metric is a proxy for the quality of the algorithm.\n",
    "\n",
    "Once the job has finished a \"Training job completed\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. We can choose any type of instance we want to host the model. In our case we chose the `ml.p3.2xlarge` instance to train, but we choose to host the model on the less expensive cpu instance, `ml.m4.xlarge``. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = od_model.deploy(initial_instance_count = 1,\n",
    "                                 instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. To do this, let us get some random image from the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# pic a random image from the validation set\n",
    "val_image_index = random.randint(0, len(validation_set))\n",
    "test_img_uri = json.loads(validation_set[val_image_index])['source-ref']\n",
    "\n",
    "l = test_img_uri[5:].split('/')\n",
    "bucket = l[0]\n",
    "key = '/'.join(l[1:])\n",
    "\n",
    "obj = s3.Object(bucket_name=bucket, key=key)\n",
    "file_name = '/tmp/test.jpg'\n",
    "\n",
    "with open(file_name, 'wb') as data:\n",
    "    obj.download_fileobj(data)\n",
    "    \n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector.content_type = 'image/jpeg'\n",
    "results = object_detector.predict(obj.get()['Body'].read())\n",
    "detections = json.loads(results)\n",
    "print (detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in a format that is similar to the .lst format with an addition of a confidence score for each detected object. The format of the output can be represented as `[class_index, confidence_score, xmin, ymin, xmax, ymax]`. Typically, we don't consider low-confidence predictions.\n",
    "\n",
    "We have provided additional script to easily visualize the detection outputs. You can visualize the high-confidence predictions with bounding box by filtering out low-confidence detections using the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = dict()\n",
    "        for det in dets:\n",
    "            (klass, score, x0, y0, x1, y1) = det\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin = int(x0 * width)\n",
    "            ymin = int(y0 * height)\n",
    "            xmax = int(x1 * width)\n",
    "            ymax = int(y1 * height)\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "            class_name = str(cls_id)\n",
    "            if classes and len(classes) > cls_id:\n",
    "                class_name = classes[cls_id]\n",
    "            plt.gca().text(xmin, ymin - 2,\n",
    "                            '{:s} {:.3f}'.format(class_name, score),\n",
    "                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                    fontsize=12, color='white')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['1', '2', '3']\n",
    "\n",
    "# Setting a threshold 0.20 will only plot detection results that have a confidence score greater than 0.20.\n",
    "threshold = 0.20\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections['prediction'], object_categories, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for AWS DeepLens\n",
    "DeepLens requires some previous manipulation of the model in order to work. For this we need the model artifact and the MXNet deploy script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model artifact\n",
    "!aws s3 cp '{od_model.model_data}' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the contents and rename it for the deployment script\n",
    "!mkdir -p model\n",
    "!tar -xvzf model.tar.gz --directory model --overwrite\n",
    "!mv model/model_algo_1-0000.params model/ssd_resnet50_300-0000.params\n",
    "!mv model/model_algo_1-symbol.json model/ssd_resnet50_300-symbol.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the script from GitHub\n",
    "!git clone https://github.com/apache/incubator-mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script with the right parameters\n",
    "!python incubator-mxnet/example/ssd/deploy.py --network resnet50 --num-class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the deployable model and copy it to the DeepLens bucket\n",
    "!tar -cvzf patched_model.tar.gz -C model \\\n",
    "./deploy_ssd_resnet50_300-0000.params \\\n",
    "./deploy_ssd_resnet50_300-symbol.json \\\n",
    "./hyperparams.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the patched model to the DeepLens bucket\n",
    "deeplens_bucket = '<your-deeplens-bucket-for-models>'\n",
    "!aws s3 cp patched_model.tar.gz s3://{deeplens_bucket}/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy\n",
    "\n",
    "Go to the DeepLens Console and create a project, import an external model using the S3 Uri for the patched_model.tar.gz, then deploy it to the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(object_detector.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
